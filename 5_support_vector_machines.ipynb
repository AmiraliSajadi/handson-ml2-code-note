{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "5_support_vector_machines.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMh63mA4sSoxW3DshSA16rF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AmiraliSajadi/handson-ml2-code-note/blob/main/5_support_vector_machines.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Support Vector Machines"
      ],
      "metadata": {
        "id": "7nI9xLhHMRGe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "5qYgqfUW7MHJ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "from sklearn import datasets\n",
        "from sklearn.datasets import make_moons\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.svm import SVC"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Linear SVM"
      ],
      "metadata": {
        "id": "l7DyJAkqMXl6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "iris = datasets.load_iris()"
      ],
      "metadata": {
        "id": "fvUq1Aw1FXbW"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iris.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Io_gdRQ7VM5S",
        "outputId": "2dd254a5-f444-411e-fc3c-af85f735fdb6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename', 'data_module'])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "iris[\"feature_names\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-JXEoDWvFy5N",
        "outputId": "0bcfcf1d-40c7-4267-88d0-aeffc339d8ee"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['sepal length (cm)',\n",
              " 'sepal width (cm)',\n",
              " 'petal length (cm)',\n",
              " 'petal width (cm)']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "iris[\"target_names\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MFwyFP3hF1FG",
        "outputId": "1b590b31-19e5-41d9-9a2a-2d010eaa3815"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['setosa', 'versicolor', 'virginica'], dtype='<U10')"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = iris[\"data\"][:, (2, 3)]    # petal length, petal width\n",
        "y = (iris[\"target\"] == 2).astype(np.float64)    # Iris virginica"
      ],
      "metadata": {
        "id": "af1KtMYsLp4p"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "C is a hyperparameter for SVM models. Larger C is a larger margin for classification (a larger street - page 155) but it can also mean better generalization for the model. </br>\n",
        "* If the SVM model is overfit, one way to regularize the model is to reduce C hyperparameter."
      ],
      "metadata": {
        "id": "FUHw2T8vWSPS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "svm_clf = Pipeline([\n",
        "                    (\"scalar\", StandardScaler()),\n",
        "                    (\"linear_svc\", LinearSVC(C=1, loss=\"hinge\")),\n",
        "  ])"
      ],
      "metadata": {
        "id": "mxZ_02tFLqOu"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "svm_clf.fit(X, y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3A-6MDpiMBAN",
        "outputId": "587bb1c3-cbcb-42a8-9180-3a75b15d6e7f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(steps=[('scalar', StandardScaler()),\n",
              "                ('linear_svc', LinearSVC(C=1, loss='hinge'))])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Making Predictions </br> The model should now be able to predict whether we got virginica or not based on petal length and width with whatever value we through at it. looking at the data, we see that 5.5, 1.7 values should belong to virginica. So we expect a 1:"
      ],
      "metadata": {
        "id": "BDIoEj0yMPKm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "svm_clf.predict([[5.5, 1.7]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-AhrB9O0MDJ-",
        "outputId": "7766562e-89a1-43a9-b4b4-7956a29f873d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1.])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nuice!"
      ],
      "metadata": {
        "id": "NFCIAsrmYKfV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "big statement with drum rolls:</br>**Unlike LR classifiers SVM classifiers don't put out probabilities for each of the classes.**"
      ],
      "metadata": {
        "id": "kOUAwIOFYQsI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pointers**:\n",
        "1. Using LinearSVC is the same as using SVC(kernel=\"linear\")\n",
        "2. Using LinearSVC is the same as using SGDClassifier(loss=\"hinge\", alpha=1/(m*C))\n",
        "3. SGDClassifier uses regular SGD to traina linear SVC classifier. It doesn't converge as fast as LinearSVC but it's useful for online classification or huge datsets that don't fit in memroy in one go (out-of-core training).\n",
        "4. To use LinearSVC you should center the training set (subtract its mean) because it regularizes the bias term. This happens automatically if you scale the data with StandardScaler.\n",
        "5. Always set the LinearSVC's *loss* to \"hinge\".\n",
        "6. Always set the LinearSVC's *dual* hyperparameter to false unless you have more features than training instances.\n"
      ],
      "metadata": {
        "id": "q8i06LvJZb3R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Nonlinear SVM Classification"
      ],
      "metadata": {
        "id": "yb3G5rF6Oa6u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's be real. Most datasets are not linearly separable. One way of handling these datasets is to add more features like polynomial features (chapter 4).\n",
        "moons dataset is a toy dataset for binary classification that we'll be using here:"
      ],
      "metadata": {
        "id": "aWehQNt4iBhn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = make_moons(n_samples=100, noise=0.15)"
      ],
      "metadata": {
        "id": "ukXEpTjuOeo_"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we use 3rd degree polynomial because moons dataset is in the sahpe of two\n",
        "# interleaving half circles\n",
        "\n",
        "polynomial_svm_clf = Pipeline([\n",
        "                       (\"poly_features\", PolynomialFeatures(degree=3)),\n",
        "                       (\"scalar\", StandardScaler()),\n",
        "                       (\"svm_clf\", LinearSVC(C=10, loss=\"hinge\")),\n",
        "    ])"
      ],
      "metadata": {
        "id": "3A5fp0mpQYam"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "polynomial_svm_clf.fit(X, y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OLRCegz9Q0I-",
        "outputId": "3acfe1fd-3026-4850-ca3f-abce46824065"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(steps=[('poly_features', PolynomialFeatures(degree=3)),\n",
              "                ('scalar', StandardScaler()),\n",
              "                ('svm_clf', LinearSVC(C=10, loss='hinge'))])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And remember that you can use the polynomial with any model (not just SVMs)"
      ],
      "metadata": {
        "id": "vm4HmWA_lPE5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's the thing: High degree polynomial is too many features (and a slow model) and low degree polynomial doesn't handle complex datasets well. Solution?</br> **Kernel Trick**: this concept is basically explained as mathematical magic (and nothing more). Kernel trick allows you to get results as if you've added the polynomial features without actually adding them. That is better results With faster computation. We use SVC for it. Here's the implementation:"
      ],
      "metadata": {
        "id": "0fH2roh8ktiG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# coef0 controls how much model is influenced by high degree vs low-degree polynomials\n",
        "poly_kernel_svm_clf = Pipeline([\n",
        "                                ('scalar', StandardScaler()),\n",
        "                                ('svm_clf', SVC(kernel=\"poly\", degree=3, coef0=1, C=5))\n",
        "])"
      ],
      "metadata": {
        "id": "kxXFF9l0lhBN"
      },
      "execution_count": 21,
      "outputs": []
    }
  ]
}