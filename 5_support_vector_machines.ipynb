{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "5_support_vector_machines.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOcT9Fc5bNGsX0P67WsD7Ji",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AmiraliSajadi/handson-ml2-code-note/blob/main/5_support_vector_machines.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Support Vector Machines"
      ],
      "metadata": {
        "id": "7nI9xLhHMRGe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "5qYgqfUW7MHJ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "from sklearn import datasets\n",
        "from sklearn.datasets import make_moons\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
        "from sklearn.svm import LinearSVC, SVC, LinearSVR, SVR"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Linear SVM"
      ],
      "metadata": {
        "id": "l7DyJAkqMXl6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "iris = datasets.load_iris()"
      ],
      "metadata": {
        "id": "fvUq1Aw1FXbW"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iris.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Io_gdRQ7VM5S",
        "outputId": "0c406449-0835-4d1d-b4db-a95bb490cb5a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename', 'data_module'])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "iris[\"feature_names\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-JXEoDWvFy5N",
        "outputId": "265fb418-ac90-4312-dc25-f11ebd0b16c6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['sepal length (cm)',\n",
              " 'sepal width (cm)',\n",
              " 'petal length (cm)',\n",
              " 'petal width (cm)']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "iris[\"target_names\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MFwyFP3hF1FG",
        "outputId": "47d01e3e-bcc3-4178-b6d1-fd82784e00ad"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['setosa', 'versicolor', 'virginica'], dtype='<U10')"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = iris[\"data\"][:, (2, 3)]    # petal length, petal width\n",
        "y = (iris[\"target\"] == 2).astype(np.float64)    # Iris virginica"
      ],
      "metadata": {
        "id": "af1KtMYsLp4p"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "C is a hyperparameter for SVM models. Larger C is a larger margin for classification (a larger street - page 155) but it can also mean better generalization for the model. </br>\n",
        "* If the SVM model is overfit, one way to regularize the model is to reduce C hyperparameter."
      ],
      "metadata": {
        "id": "FUHw2T8vWSPS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "svm_clf = Pipeline([\n",
        "                    (\"scalar\", StandardScaler()),\n",
        "                    (\"linear_svc\", LinearSVC(C=1, loss=\"hinge\")),\n",
        "  ])"
      ],
      "metadata": {
        "id": "mxZ_02tFLqOu"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "svm_clf.fit(X, y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3A-6MDpiMBAN",
        "outputId": "e650fcca-c393-4600-aa17-6067aff189c8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(steps=[('scalar', StandardScaler()),\n",
              "                ('linear_svc', LinearSVC(C=1, loss='hinge'))])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Making Predictions </br> The model should now be able to predict whether we got virginica or not based on petal length and width with whatever value we through at it. looking at the data, we see that 5.5, 1.7 values should belong to virginica. So we expect a 1:"
      ],
      "metadata": {
        "id": "BDIoEj0yMPKm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "svm_clf.predict([[5.5, 1.7]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-AhrB9O0MDJ-",
        "outputId": "ea60c3de-8384-48a9-875b-0e9219657ce8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1.])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nuice!"
      ],
      "metadata": {
        "id": "NFCIAsrmYKfV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "big statement with drum rolls:</br>**Unlike LR classifiers SVM classifiers don't put out probabilities for each of the classes.**"
      ],
      "metadata": {
        "id": "kOUAwIOFYQsI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pointers**:\n",
        "1. Using LinearSVC is the same as using SVC(kernel=\"linear\")\n",
        "2. Using LinearSVC is the same as using SGDClassifier(loss=\"hinge\", alpha=1/(m*C))\n",
        "3. SGDClassifier uses regular SGD to traina linear SVC classifier. It doesn't converge as fast as LinearSVC but it's useful for online classification or huge datsets that don't fit in memroy in one go (out-of-core training).\n",
        "4. To use LinearSVC you should center the training set (subtract its mean) because it regularizes the bias term. This happens automatically if you scale the data with StandardScaler.\n",
        "5. Always set the LinearSVC's *loss* to \"hinge\".\n",
        "6. Always set the LinearSVC's *dual* hyperparameter to false unless you have more features than training instances.\n"
      ],
      "metadata": {
        "id": "q8i06LvJZb3R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Nonlinear SVM Classification"
      ],
      "metadata": {
        "id": "yb3G5rF6Oa6u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's be real. Most datasets are not linearly separable. One way of handling these datasets is to add more features like polynomial features (chapter 4).\n",
        "moons dataset is a toy dataset for binary classification that we'll be using here:"
      ],
      "metadata": {
        "id": "aWehQNt4iBhn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = make_moons(n_samples=100, noise=0.15)"
      ],
      "metadata": {
        "id": "ukXEpTjuOeo_"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we use 3rd degree polynomial because moons dataset is in the sahpe of two\n",
        "# interleaving half circles\n",
        "\n",
        "polynomial_svm_clf = Pipeline([\n",
        "                       (\"poly_features\", PolynomialFeatures(degree=3)),\n",
        "                       (\"scalar\", StandardScaler()),\n",
        "                       (\"svm_clf\", LinearSVC(C=10, loss=\"hinge\")),\n",
        "    ])"
      ],
      "metadata": {
        "id": "3A5fp0mpQYam"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "polynomial_svm_clf.fit(X, y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OLRCegz9Q0I-",
        "outputId": "7204ad3e-be04-4dff-bad2-3d73e04a197c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(steps=[('poly_features', PolynomialFeatures(degree=3)),\n",
              "                ('scalar', StandardScaler()),\n",
              "                ('svm_clf', LinearSVC(C=10, loss='hinge'))])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And remember that you can use the polynomial with any model (not just SVMs)"
      ],
      "metadata": {
        "id": "vm4HmWA_lPE5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's the thing: High degree polynomial is too many features (and a slow model) and low degree polynomial doesn't handle complex datasets well. Solution?</br> **Kernel Trick**: this concept is basically explained as mathematical magic (and nothing more). Kernel trick allows you to get results as if you've added the polynomial features without actually adding them. That is better results With faster computation. We use SVC for it. Here's the implementation:"
      ],
      "metadata": {
        "id": "0fH2roh8ktiG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# coef0 controls how much model is influenced by high degree vs low-degree polynomials\n",
        "# Do consider grid searches for finding the right hyperparameters here ;)\n",
        "poly_kernel_svm_clf = Pipeline([\n",
        "                                ('scalar', StandardScaler()),\n",
        "                                ('svm_clf', SVC(kernel=\"poly\", degree=3, coef0=1, C=5))\n",
        "])"
      ],
      "metadata": {
        "id": "kxXFF9l0lhBN"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similarity Features</br>\n",
        "Using similarity features for nonlinear problems also adds features. The added features are computed with a *similarity function* that measures how much each instance resembles a particular landmark (do look into page 159 as it's super hard to summarize this one without pictures).</br> The similarity functiont that we'll use here is *Gaussian RBF*\n"
      ],
      "metadata": {
        "id": "Yw5P0RxJ47Eq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rbf_kernel_svm_clf = Pipeline([\n",
        "                               (\"scaler\", StandardScaler()),\n",
        "                               (\"svm_clf\", SVC(kernel=\"rbf\", gamma=5, C=0.001))\n",
        "])\n",
        "\n",
        "rbf_kernel_svm_clf.fit(X, y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QK0Hb37446Z4",
        "outputId": "71908316-cb57-481c-c897-fe41813e96e6"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(steps=[('scaler', StandardScaler()),\n",
              "                ('svm_clf', SVC(C=0.001, gamma=5))])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "How to choose a kernel?</br>\n",
        "Well... Always try to start with a linear kernel (LinearSVC is faster than SVC(kernel=\"linear\")). After that move on to Gaussian RBF which usually works. If that didn't work, you gotta experiment with other kernels and grid searches. You can also check out the computational complexities of SVC SGDClassifier and LinearSVC in page 162's table."
      ],
      "metadata": {
        "id": "hZzXbAPl-Swy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SVM Regression\n",
        "who said SVMs can only classify?"
      ],
      "metadata": {
        "id": "1Kf5JK1kCHtS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "svm_reg = LinearSVR(epsilon=1.5)\n",
        "svm_reg.fit(X, y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "icNJ84Pu-QoM",
        "outputId": "b5ebea16-467b-4534-9b63-ef34d3706065"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LinearSVR(epsilon=1.5)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Need nonlinear regression done? No problem! Use a kernelized SVM model:"
      ],
      "metadata": {
        "id": "JQ5_6ZQfDcET"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# the lower the C the more the regularization:\n",
        "svm_poly_reg = SVR(kernel=\"poly\", degree=2, C=100, epsilon=0.1)\n",
        "svm_poly_reg.fit(X, y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FPbyJU_3DIFT",
        "outputId": "f7deda98-0c05-4f16-fc6f-70cb56b97137"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVR(C=100, degree=2, kernel='poly')"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The rest of the chapter explains the math under the hood of the SVM models. Code's done."
      ],
      "metadata": {
        "id": "0mJh4ZVgJ4uo"
      }
    }
  ]
}